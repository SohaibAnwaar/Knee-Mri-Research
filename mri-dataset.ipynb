{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport re\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nimport pickle #for reading images\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport scipy.ndimage\nfrom skimage import measure, morphology\n\nfrom keras.layers import Conv2D, MaxPool3D, Flatten, Dense\nfrom keras.layers import Dropout, Input, BatchNormalization\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom plotly.offline import iplot, init_notebook_mode\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adadelta\n\n\nfrom keras.models import Model\n\nimport keras\n\n\n\nfrom pathlib import Path\n\nimport matplotlib.patches as patch\n\nfrom PIL import Image\nfrom skimage.transform import resize    \n\nimport cv2\n\n\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import img_to_array\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"c80a9ca5a3bb9e282da5f6a7aae9a379762ea7bc","execution":{"iopub.status.busy":"2021-10-03T15:57:55.346325Z","iopub.execute_input":"2021-10-03T15:57:55.346609Z","iopub.status.idle":"2021-10-03T15:57:59.367758Z","shell.execute_reply.started":"2021-10-03T15:57:55.346558Z","shell.execute_reply":"2021-10-03T15:57:59.367075Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# directory where the volumetric data is located\nvolumetric_data_dir = '../input/vol01'\n\n# path to metadata csv file\nmetadata_csv_path = '../input/metadata.csv'\n\n# names=True loads the interprets the first row of csv file as column names\n# 'i4' = 4 byte signed integer, 'U20' = unicode max 20 char string\nmetadata = np.genfromtxt(metadata_csv_path, delimiter=',', names=True, \n    dtype='i4,i4,i4,i4,i4,i4,i4,i4,i4,i4,U20') \n\nprint('Column names:')\nprint(metadata.dtype.names)\n\n# Select all rows where examID == 502889\nexams = metadata[metadata['examId'] == 404663]\n\nfor exam in exams:\n    vol_data_file = exam['volumeFilename']\n\n    vol_data_path = os.path.join(volumetric_data_dir, vol_data_file)\n\n    # Load data from file\n    with open(vol_data_path, 'rb') as file_handler: # Must use 'rb' as the data is binary\n        volumetric_data = pickle.load(file_handler)\n    \n    print('\\nShape of volume \"%s\":' % vol_data_path, volumetric_data.shape)\n    \n    # Get all roi slices from volume\n    z_start = exam['roiZ']\n    depth = exam['roiDepth']\n    \n    for z in range(z_start, z_start + depth):\n    \n        slice = volumetric_data[z, :, :]\n        \n        # Get roi dimensions\n        x, y, w, h = [exam[attr] for attr in ['roiX', 'roiY', 'roiWidth', 'roiHeight']]\n        \n        # Extract ROI\n        roi = slice[y:y+h, x:x+w]\n        \n        # Plot slice and roi\n        figure = plt.figure()\n        plot = plt.subplot2grid((1, 4), (0, 0), 1, 3) # This makes the slice plot larger than roi plot\n        plot.add_patch(patch.Rectangle((x, y), w, h, fill=None, color='red'))\n        plot.imshow(slice, cmap='gray')\n        plot = plt.subplot2grid((1, 4), (0, 3), 1, 1)\n        plot.imshow(roi, cmap='gray')\n        \n        plt.show()","metadata":{"_uuid":"827c4e770b223a7703321ab2060081ac5be2f59a","execution":{"iopub.status.busy":"2021-10-03T15:57:59.370791Z","iopub.execute_input":"2021-10-03T15:57:59.371032Z","iopub.status.idle":"2021-10-03T15:58:00.533342Z","shell.execute_reply.started":"2021-10-03T15:57:59.370983Z","shell.execute_reply":"2021-10-03T15:58:00.532382Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\na=[]\ndf=pd.read_csv(\"../input/metadata.csv\")\n8#df = df.sample(n=15)     # k rows\nfor MRI in df['volumeFilename']:\n    for Dir in (\"vol01\",\"vol02\",\"vol03\",\"vol04\",\"vol05\",\"vol06\",\"vol07\",\"vol08\"):\n    \n        my_file=Path(\"../input/\"+Dir+\"/\"+MRI )\n    \n        if my_file.exists():\n            a.append(my_file)\n    \n\ndf['path']=\"Image Not Here\"\nstring1=[]\nnew_df=pd.DataFrame()\nfor i in a:\n    string1.append(str(i))\nstring1\nfor b in string1:\n    for i, row in df.iterrows():\n        \n        string1=b.split('/')\n        if re.match(row['volumeFilename'],string1[3]):\n            row['path']=b\n            df1=row.to_frame()\n            df1_transposed = df1.T\n            frames = [df,df1_transposed]\n            df=pd.concat(frames)\n            break\n            ","metadata":{"_uuid":"5131e9c0ce6299af2f37d57a4acc8bb9e29744ed","execution":{"iopub.status.busy":"2021-10-03T15:58:00.534844Z","iopub.execute_input":"2021-10-03T15:58:00.535295Z","iopub.status.idle":"2021-10-03T15:58:32.561584Z","shell.execute_reply.started":"2021-10-03T15:58:00.535112Z","shell.execute_reply":"2021-10-03T15:58:32.560462Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Droping records for which we dont have Images\ndf=df[df['path']!=\"Image Not Here\"]\nsizes=df.aclDiagnosis\nx=pd.value_counts(sizes)\n\nidx = pd.Int64Index([0,1,2])\nPie_Chart_df = pd.DataFrame(index = idx, data =(x))\nplt.pie(Pie_Chart_df.aclDiagnosis,labels=Pie_Chart_df.index)\n","metadata":{"_uuid":"fb40f867cb39ed7e7274758ad90db260ad5d63db","execution":{"iopub.status.busy":"2021-10-03T15:58:32.566017Z","iopub.execute_input":"2021-10-03T15:58:32.566451Z","iopub.status.idle":"2021-10-03T15:58:32.760302Z","shell.execute_reply.started":"2021-10-03T15:58:32.566253Z","shell.execute_reply":"2021-10-03T15:58:32.757119Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=Pie_Chart_df.index,y=Pie_Chart_df.aclDiagnosis)","metadata":{"_uuid":"6a5cd92e96716fcedeed50b7575fa1e9d3533b1b","execution":{"iopub.status.busy":"2021-10-03T15:58:32.762569Z","iopub.execute_input":"2021-10-03T15:58:32.762916Z","iopub.status.idle":"2021-10-03T15:58:33.054023Z","shell.execute_reply.started":"2021-10-03T15:58:32.762830Z","shell.execute_reply":"2021-10-03T15:58:33.053083Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"This is our **ROI** so that we will focous only this part","metadata":{"_uuid":"cf17e1aed20366ed090839fc5370e0710d2f3371"}},{"cell_type":"code","source":"#taking record 6 to only visulaize the image\nwith open(df['path'].iloc[6], 'rb') as file_handler: # Must use 'rb' as the data is binary\n    volumetric_data = pickle.load(file_handler)\nimg=volumetric_data[df['roiZ'].iloc[6], :, :]\n\nx=df[\"roiX\"].iloc[6]\ny=df[\"roiY\"].iloc[6]\nw=df[\"roiWidth\"].iloc[6]\nh=df[\"roiHeight\"].iloc[6]\nimage_array=img[y:y+h, x:x+w]\nfigure = plt.figure()\nplot = plt.subplot2grid((1, 4), (0, 3), 1, 1)\n        \nplot.imshow(image_array, cmap='gray')\n        \n        \n        \nplt.show()\nprint(image_array.shape)","metadata":{"_uuid":"85274fe13204efac736457057c86b7d87bcf4b30","execution":{"iopub.status.busy":"2021-10-03T15:58:33.058534Z","iopub.execute_input":"2021-10-03T15:58:33.058955Z","iopub.status.idle":"2021-10-03T15:58:33.440806Z","shell.execute_reply.started":"2021-10-03T15:58:33.058779Z","shell.execute_reply":"2021-10-03T15:58:33.439894Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"First we will classify** Binary data** lets see what will be the results\nNow Binary Calssifying The clases \n* 1==> Ruptured\n* 0==> Healthy","metadata":{"_uuid":"6ee8967b6780e4dc8e7101a62ba3f5a9812bd29c"}},{"cell_type":"code","source":"#making binary dataset\nnew_df0=df[df.aclDiagnosis==0]\n#selecting equal number of labels from dataset\n\nnew_df0=new_df0.sample(200) #200 class having Healthy knee\nnew_df1=df[df.aclDiagnosis!=0] #almost 200 class having ruptured knee\nnew_df1['aclDiagnosis']=new_df1.aclDiagnosis.replace(2,1) \nframes = [new_df1, new_df0]\nnew_df = pd.concat(frames)","metadata":{"_uuid":"c47b32f013606884abe3326cf93314e9599d41ea","execution":{"iopub.status.busy":"2021-10-03T15:58:33.445220Z","iopub.execute_input":"2021-10-03T15:58:33.447457Z","iopub.status.idle":"2021-10-03T15:58:33.591346Z","shell.execute_reply.started":"2021-10-03T15:58:33.445479Z","shell.execute_reply":"2021-10-03T15:58:33.590643Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sizes=new_df.aclDiagnosis\nx=pd.value_counts(sizes)\n\nidx = pd.Int64Index([0,1])\nPie_Chart_df = pd.DataFrame(index = idx, data =(x))\nplt.pie(Pie_Chart_df.aclDiagnosis,labels=Pie_Chart_df.index)\n#their is one forth part of binary positive labels in all dataset","metadata":{"_uuid":"bdcde8bc4e8b43cd72e86af7b3c27a5415bc2068","execution":{"iopub.status.busy":"2021-10-03T15:58:33.592417Z","iopub.execute_input":"2021-10-03T15:58:33.592877Z","iopub.status.idle":"2021-10-03T15:58:33.690072Z","shell.execute_reply.started":"2021-10-03T15:58:33.592815Z","shell.execute_reply":"2021-10-03T15:58:33.689325Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom skimage.transform import resize       \nimages_path=new_df['path']\nimage_list = []\n\nfor i in range(len(new_df)):\n    with open(new_df['path'].iloc[i], 'rb') as file_handler: # Must use 'rb' as the data is binary\n        image_array = pickle.load(file_handler)\n    img=image_array[new_df['roiZ'].iloc[i], :, :]\n    x=new_df[\"roiX\"].iloc[i]\n    y=new_df[\"roiY\"].iloc[i]\n    w=new_df[\"roiWidth\"].iloc[i]\n    h=new_df[\"roiHeight\"].iloc[i]\n    image_array=img[y:y+h, x:x+w]\n    \n    imageB_array = resize(image_array, (90, 90))\n    image_list.append(imageB_array)\n    \nimg_list=np.asarray(image_list)\n\nY=new_df.aclDiagnosis\nY=np.asarray(Y)\nY = to_categorical(Y, num_classes=2)\nimg_list = img_list.reshape(-1, 90,90,1)\nimg_list.shape\n","metadata":{"_uuid":"c47a0d2990d4a0ece8634d43d59f0596d539dc15","execution":{"iopub.status.busy":"2021-10-03T15:58:33.693054Z","iopub.execute_input":"2021-10-03T15:58:33.693557Z","iopub.status.idle":"2021-10-03T15:59:19.530233Z","shell.execute_reply.started":"2021-10-03T15:58:33.693508Z","shell.execute_reply":"2021-10-03T15:59:19.529397Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    img_list,Y, test_size=0.30, random_state=42)","metadata":{"_uuid":"50ee07c407c1963150846380b3e57cd7f9d2d159","execution":{"iopub.status.busy":"2021-10-03T15:59:19.533157Z","iopub.execute_input":"2021-10-03T15:59:19.533596Z","iopub.status.idle":"2021-10-03T15:59:19.565372Z","shell.execute_reply.started":"2021-10-03T15:59:19.533415Z","shell.execute_reply":"2021-10-03T15:59:19.564445Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(X_train[0], (90,90))\nplt.imshow(curr_img, cmap='gray')\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(X_test[0], (90,90))\nplt.imshow(curr_img, cmap='gray')","metadata":{"_uuid":"753d088664c48da3068808d3c4791e38ab38c5c6","execution":{"iopub.status.busy":"2021-10-03T15:59:19.566559Z","iopub.execute_input":"2021-10-03T15:59:19.566831Z","iopub.status.idle":"2021-10-03T15:59:19.944763Z","shell.execute_reply.started":"2021-10-03T15:59:19.566785Z","shell.execute_reply":"2021-10-03T15:59:19.943774Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"This is Our **Deep Nueral Network Model** On which we are going to Train our Images","metadata":{"_uuid":"b0395698dc27b1412d2db596249f40ea1142e4d6"}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.core import Activation\nfrom keras import regularizers\nfrom keras.layers.convolutional import MaxPooling2D","metadata":{"_uuid":"286987814b9d2b996a7fb5719bb84de3f2abc8d4","execution":{"iopub.status.busy":"2021-10-03T15:59:19.949275Z","iopub.execute_input":"2021-10-03T15:59:19.949697Z","iopub.status.idle":"2021-10-03T15:59:19.958724Z","shell.execute_reply.started":"2021-10-03T15:59:19.949530Z","shell.execute_reply":"2021-10-03T15:59:19.957608Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n\ndef model(classes):\n    model=Sequential()\n        \n        # conv2d set  =====> Conv2d====>relu=====>MaxPooling\n    model.add(Conv2D(20,(5,5),padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    model.add(Conv2D(20,(5,5),padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    model.add(Conv2D(20,(5,5),padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n   \n    model.add(Flatten())\n    \n    model.add(Dense(64))\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.3))    \n    model.add(Dense(32))\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.3))   \n        \n    model.add(Dense(classes))\n    model.add(Activation(\"softmax\"))\n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(lr=0.00001, decay=0),\n                  metrics=['accuracy', f1_m,precision_m, recall_m])\n    return model","metadata":{"_uuid":"0164d40242f7d1f9a08f5ff73d22509e81c2bdf7","execution":{"iopub.status.busy":"2021-10-03T17:00:10.819439Z","iopub.execute_input":"2021-10-03T17:00:10.819760Z","iopub.status.idle":"2021-10-03T17:00:10.834813Z","shell.execute_reply.started":"2021-10-03T17:00:10.819687Z","shell.execute_reply":"2021-10-03T17:00:10.833129Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Nice its giving **70% accuracy on binary data** lets apply some **augmentation** on the images to increase the number of images lets see how much it effect on our model","metadata":{"_uuid":"688da6aabee81306aaf1435b9bccad900439c6fa","trusted":true}},{"cell_type":"code","source":"Binary_model = model(3)\n# Binary_model.summary()","metadata":{"_uuid":"10c05ae21e5e88916ac0a0c0b0ff3f4af3477659","execution":{"iopub.status.busy":"2021-10-03T16:03:43.173777Z","iopub.execute_input":"2021-10-03T16:03:43.174085Z","iopub.status.idle":"2021-10-03T16:03:43.195932Z","shell.execute_reply.started":"2021-10-03T16:03:43.174030Z","shell.execute_reply":"2021-10-03T16:03:43.195340Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Now Lets see our model accuracy on Categorical Data\n*     0:    Healthy\n*     1:    half_Raptured\n*     2:    Full_Raptured\n    \n    ","metadata":{"_uuid":"03f4345de1d50db6ec14779596be218e8b068ec8"}},{"cell_type":"code","source":"\n   \nimages_path=df['path']\nimage_list = []\n\nfor i in range(len(df)):\n    with open(df['path'].iloc[i], 'rb') as file_handler: # Must use 'rb' as the data is binary\n        image_array = pickle.load(file_handler)\n    img=image_array[df['roiZ'].iloc[i], :, :]\n    x=df[\"roiX\"].iloc[i]\n    y=df[\"roiY\"].iloc[i]\n    w=df[\"roiWidth\"].iloc[i]\n    h=df[\"roiHeight\"].iloc[i]\n    image_array=img[y:y+h, x:x+w]\n    \n    imageB_array = resize(image_array, (90, 90))\n    image_list.append(imageB_array)\n    \nimg_list=np.asarray(image_list)\n\nY=df.aclDiagnosis\nY=np.asarray(Y)\nY = to_categorical(Y, num_classes=3)     \n","metadata":{"_uuid":"ce4ca11534c65ce52fbbf4dd4e7292903f032b12","execution":{"iopub.status.busy":"2021-10-03T16:03:44.708015Z","iopub.execute_input":"2021-10-03T16:03:44.708359Z","iopub.status.idle":"2021-10-03T16:05:06.305993Z","shell.execute_reply.started":"2021-10-03T16:03:44.708312Z","shell.execute_reply":"2021-10-03T16:05:06.305086Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"img_list = img_list.reshape(-1, 90,90,1)\nimg_list.shape","metadata":{"_uuid":"0fe26ec7119f82f1199e240c3b8a81749d9f4f52","execution":{"iopub.status.busy":"2021-10-03T16:05:06.307450Z","iopub.execute_input":"2021-10-03T16:05:06.307942Z","iopub.status.idle":"2021-10-03T16:05:06.314016Z","shell.execute_reply.started":"2021-10-03T16:05:06.307890Z","shell.execute_reply":"2021-10-03T16:05:06.313336Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    img_list,Y, test_size=0.10, random_state=42)\n","metadata":{"_uuid":"d1706d773e35bbc64741d1b8d9d30188918f5a9c","execution":{"iopub.status.busy":"2021-10-03T16:05:06.315470Z","iopub.execute_input":"2021-10-03T16:05:06.316038Z","iopub.status.idle":"2021-10-03T16:05:06.385564Z","shell.execute_reply.started":"2021-10-03T16:05:06.315989Z","shell.execute_reply":"2021-10-03T16:05:06.384854Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(X_train[0], (90,90))\nplt.imshow(curr_img, cmap='gray')\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(X_test[0], (90,90))\nplt.imshow(curr_img, cmap='gray')","metadata":{"_uuid":"d458bcabe80a781767a5675dbe203557a3b3ad6a","execution":{"iopub.status.busy":"2021-10-03T16:05:06.386826Z","iopub.execute_input":"2021-10-03T16:05:06.387126Z","iopub.status.idle":"2021-10-03T16:05:06.746588Z","shell.execute_reply.started":"2021-10-03T16:05:06.387082Z","shell.execute_reply":"2021-10-03T16:05:06.745642Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\nmodel=model(3)\nhistory =model.fit(X_train, y_train, epochs=10)\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n\n\n(custom_loss, \ncustom_accuracy, \ncustom_f1_score, custom_precision, custom_recall) = model.evaluate(X_test, y_test, verbose=1)\n\n\n\nprint(f\"Custom Recall {custom_recall}\")\nprint(f\"Custom accuracy {custom_accuracy}\")\nprint(f\"Custom loss {custom_loss}\")\nprint(f\"Custom precision {custom_precision}\")\nprint(f\"Custom f1_score {custom_f1_score}\")","metadata":{"_uuid":"d1fd2acb0eb0ec3feee0391c2989103d7e2e65de","execution":{"iopub.status.busy":"2021-10-03T16:05:06.747972Z","iopub.execute_input":"2021-10-03T16:05:06.748224Z","iopub.status.idle":"2021-10-03T16:05:15.225643Z","shell.execute_reply.started":"2021-10-03T16:05:06.748180Z","shell.execute_reply":"2021-10-03T16:05:15.224699Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### scores = model.evaluate(X_test,y_test)\n\nprint('Loss: %.3f' % scores[0])\nprint('Accuracy: %.3f' % scores[1])","metadata":{"_uuid":"45904a31421e45d7f3486211f59e7e8e2e5019fd","trusted":true}},{"cell_type":"code","source":"x=model.predict(X_test)","metadata":{"_uuid":"68cd4fcb48968c8c4bc2586e146138bb10515b8a","execution":{"iopub.status.busy":"2021-10-03T16:05:15.230340Z","iopub.execute_input":"2021-10-03T16:05:15.230768Z","iopub.status.idle":"2021-10-03T16:05:15.343955Z","shell.execute_reply.started":"2021-10-03T16:05:15.230596Z","shell.execute_reply":"2021-10-03T16:05:15.343135Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#images which are labeled as Half Raptured\nfor i in range(1,74):\n    ind = np.argmax(x[i])\n    \n    if ind == 1 : print(\"Half Raptured Image\" + str(i))\n    if ind == 2 : print(\"Full Ruptured Image\")","metadata":{"_uuid":"c6ac08db93e81c3a7fa3953f904ce19363535f9d","execution":{"iopub.status.busy":"2021-10-03T16:05:15.345231Z","iopub.execute_input":"2021-10-03T16:05:15.345494Z","iopub.status.idle":"2021-10-03T16:05:15.350408Z","shell.execute_reply.started":"2021-10-03T16:05:15.345448Z","shell.execute_reply":"2021-10-03T16:05:15.349704Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import random\ni=random.choice([1,2,3,4,5])\nplt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(X_train[0], (90,90))\nplt.imshow(curr_img, cmap='gray')\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(X_test[i], (90,90))\nplt.imshow(curr_img, cmap='gray')\n\nPredicted_label = np.argmax(x[i])\nif Predicted_label == 0 : print(\"Healthy Image\")\nif Predicted_label == 1 : print(\"Half Raptured Image\")\nif Predicted_label == 2 : print(\"Full Ruptured Image\")\n    \n    \nTrue_Label = np.argmax(y_test[i])\nif True_Label == 0 : print(\"Healthy Image\")\nif True_Label == 1 : print(\"Half Raptured Image\")\nif True_Label == 2 : print(\"Full Ruptured Image\")","metadata":{"_uuid":"44f5dd127db50a1c1f6d28231210816bab9853c5","execution":{"iopub.status.busy":"2021-10-03T16:05:57.985675Z","iopub.execute_input":"2021-10-03T16:05:57.986009Z","iopub.status.idle":"2021-10-03T16:05:58.346638Z","shell.execute_reply.started":"2021-10-03T16:05:57.985949Z","shell.execute_reply":"2021-10-03T16:05:58.345823Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**True Labels**","metadata":{"_uuid":"52fc1b98432aadbfbbcdab0eca4f844eef005039"}},{"cell_type":"code","source":"for i in range(1,74):\n    ind = np.argmax(y_test[i])\n\n    if ind == 1 : print(\"Half Raptured Image \" + str(i))\n    if ind == 2 : print(\"Full Ruptured Image \" + str(i))","metadata":{"_uuid":"b4b74a9ea1c2a8f9f6bf2d59a5ad5bfec6d1d522","execution":{"iopub.status.busy":"2021-10-03T16:05:24.191152Z","iopub.execute_input":"2021-10-03T16:05:24.191683Z","iopub.status.idle":"2021-10-03T16:05:24.200776Z","shell.execute_reply.started":"2021-10-03T16:05:24.191405Z","shell.execute_reply":"2021-10-03T16:05:24.199590Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# InceptionV3","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom skimage.transform import resize       \nimages_path=new_df['path']\nimage_list = []\n\nfor i in range(len(new_df)):\n    with open(new_df['path'].iloc[i], 'rb') as file_handler: # Must use 'rb' as the data is binary\n        image_array = pickle.load(file_handler)\n    img=image_array[new_df['roiZ'].iloc[i], :, :]\n    x=new_df[\"roiX\"].iloc[i]\n    y=new_df[\"roiY\"].iloc[i]\n    w=new_df[\"roiWidth\"].iloc[i]\n    h=new_df[\"roiHeight\"].iloc[i]\n    image_array=img[y:y+h, x:x+w]\n    \n    imageB_array = resize(image_array, (224, 224,3))\n    image_list.append(imageB_array)\n    \nimg_list=np.asarray(image_list)\n\nY=new_df.aclDiagnosis\nY=np.asarray(Y)\nY = to_categorical(Y, num_classes=2)\nimg_list = img_list.reshape(-1, 224,224,3)\nimg_list.shape\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    img_list,Y, test_size=0.10, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-03T16:50:05.379375Z","iopub.execute_input":"2021-10-03T16:50:05.379655Z","iopub.status.idle":"2021-10-03T16:50:30.539076Z","shell.execute_reply.started":"2021-10-03T16:50:05.379601Z","shell.execute_reply":"2021-10-03T16:50:30.538348Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.layers import Input\nimport tensorflow as tf\n\n\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))\n\nmodel = InceptionV3(input_tensor=input_tensor, weights=None, include_top=True, classes=2)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', f1_m,precision_m, recall_m])\n\n# train the model on the new data for a few epochs\nmodel.fit(X_train, y_train, epochs=10)","metadata":{"_uuid":"d668961e526099f37414601d1fe4a5a1a25332ae","execution":{"iopub.status.busy":"2021-10-03T17:00:22.307120Z","iopub.execute_input":"2021-10-03T17:00:22.307427Z","iopub.status.idle":"2021-10-03T17:08:51.554273Z","shell.execute_reply.started":"2021-10-03T17:00:22.307375Z","shell.execute_reply":"2021-10-03T17:08:51.553466Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"\n\n(inception_loss, \ninception_accuracy, \ninception_f1_score, inception_precision, inception_recall) = model.evaluate(X_test, y_test, verbose=1)\n\n\n\nprint(f\"Inception Recall {inception_recall}\")\nprint(f\"Inception accuracy {inception_accuracy}\")\nprint(f\"Inception loss {inception_loss}\")\nprint(f\"Inception precision {inception_precision}\")\nprint(f\"Inception f1_score {inception_f1_score}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-03T17:11:57.383419Z","iopub.execute_input":"2021-10-03T17:11:57.383723Z","iopub.status.idle":"2021-10-03T17:11:57.650989Z","shell.execute_reply.started":"2021-10-03T17:11:57.383673Z","shell.execute_reply":"2021-10-03T17:11:57.649709Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# Mobilenet","metadata":{}},{"cell_type":"code","source":"# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))\n\nmodel = MobileNet(input_tensor=input_tensor, weights=None, include_top=True, classes=2)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', f1_m,precision_m, recall_m])\n\n# train the model on the new data for a few epochs\nmodel.fit(img_list, Y, epochs=10)","metadata":{"_uuid":"9be08885e7b5797c02dbc3668a199f994059c2d3","execution":{"iopub.status.busy":"2021-10-03T17:12:03.889165Z","iopub.execute_input":"2021-10-03T17:12:03.889511Z","iopub.status.idle":"2021-10-03T17:20:48.990047Z","shell.execute_reply.started":"2021-10-03T17:12:03.889453Z","shell.execute_reply":"2021-10-03T17:20:48.989261Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"\n(mobilenet_loss, \nmobilenet_accuracy, \nmobilenet_f1_score, mobilenet_precision, mobilenet_recall) = model.evaluate(X_test, y_test, verbose=1)\n\n\n\nprint(f\"Mobilenet Recall {mobilenet_recall}\")\nprint(f\"Mobilenet accuracy {mobilenet_accuracy}\")\nprint(f\"Mobilenet loss {mobilenet_loss}\")\nprint(f\"Mobilenet precision {mobilenet_precision}\")\nprint(f\"Mobilenet f1_score {mobilenet_f1_score}\")","metadata":{"_uuid":"c2fc4e1f3ea96ac9bd8e275d4d3fe9de7b69ecb8","execution":{"iopub.status.busy":"2021-10-03T17:20:48.992505Z","iopub.execute_input":"2021-10-03T17:20:48.992778Z","iopub.status.idle":"2021-10-03T17:21:43.951847Z","shell.execute_reply.started":"2021-10-03T17:20:48.992730Z","shell.execute_reply":"2021-10-03T17:21:43.951241Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.resnet50 import ResNet50\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))\n\nmodel = ResNet50(input_tensor=input_tensor, weights=None, include_top=True, classes=2)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy', f1_m,precision_m, recall_m])\n\n# train the model on the new data for a few epochs\nmodel.fit(img_list, Y, epochs=10)","metadata":{"_uuid":"1ed305c74c4e48f8f6d9b6e10309f11d8cc1d5d2","execution":{"iopub.status.busy":"2021-10-03T17:28:08.872335Z","iopub.execute_input":"2021-10-03T17:28:08.872635Z","iopub.status.idle":"2021-10-03T17:38:22.580851Z","shell.execute_reply.started":"2021-10-03T17:28:08.872582Z","shell.execute_reply":"2021-10-03T17:38:22.580042Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"\n(resnet_loss, \nresnet_accuracy, \nresnet_f1_score, resnet_precision, resnet_recall) = model.evaluate(X_test, y_test, verbose=1)\n\n\n\nprint(f\"Resnet Recall {resnet_recall}\")\nprint(f\"Resnet accuracy {resnet_accuracy}\")\nprint(f\"Resnet loss {resnet_loss}\")\nprint(f\"Resnet precision {resnet_precision}\")\nprint(f\"Resnet f1_score {resnet_f1_score}\")","metadata":{"_uuid":"72cf9a90319602b8f5de209df3a35fffe5b94e3a","execution":{"iopub.status.busy":"2021-10-03T17:38:22.584959Z","iopub.execute_input":"2021-10-03T17:38:22.587172Z","iopub.status.idle":"2021-10-03T17:39:27.274598Z","shell.execute_reply.started":"2021-10-03T17:38:22.587117Z","shell.execute_reply":"2021-10-03T17:39:27.274015Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nmatrics=['Recall', 'Accuracy', 'Precesion','Loss','F1-Score']\n\nfig = go.Figure(data=[\n    go.Bar(name='Mobilenet', x=matrics, y=[mobilenet_loss,  mobilenet_accuracy, mobilenet_f1_score, mobilenet_precision, mobilenet_recall]),\n    go.Bar(name='Inception', x=matrics, y=[inception_loss, inception_accuracy, inception_f1_score, inception_precision, inception_recall]),\n    go.Bar(name='Resnet Net', x=matrics, y=[resnet_loss, resnet_accuracy, resnet_f1_score, resnet_precision, resnet_recall]),\n    go.Bar(name='Custom Net', x=matrics, y=[custom_loss, custom_accuracy, custom_f1_score, custom_precision, custom_recall])\n])\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","metadata":{"_uuid":"a3b4b290a3ee866e5be8ee84f3e0c493869472c0","execution":{"iopub.status.busy":"2021-10-03T17:39:46.983342Z","iopub.execute_input":"2021-10-03T17:39:46.983629Z","iopub.status.idle":"2021-10-03T17:39:47.011557Z","shell.execute_reply.started":"2021-10-03T17:39:46.983576Z","shell.execute_reply":"2021-10-03T17:39:47.010247Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4234b702932a6b406681254e39553c2c70ae31c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e8492a78e8214428ce264879b45f6e990563cf7c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"5f10a1421bdce8e11be1741690eeab4518b040b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"3b32eacc0db2a0044379a5ee30463da29b7808db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d77e530e92c77288b8423778d53ee3c7cf88e5e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"db42c31c7bae9758d3152d9795c6e58fe581c224","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"55fb83b6fde4864bf76032824e220e5406a89db1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"916fb4faaa9e347e26a26091783c2469b8bf54c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4aac72417f0c01f82018f2f54f8b3dd593cbcb45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8107b9a1a53d4fce2229869ee9e5f5a4f08f360f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"1b67966eec5925b64ed0546775f15d54f62c6046","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"65f893358a499c6116f0752685a1921404fbcd03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"81068bef2b89aba4cd2d34c5da90eb000c7ebf65","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"700112844c6a995e63cbfc4e3b3bf55a804e5089","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ce4b06d88e0951ce497056c5900fa63916490964","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b0ac9f401f5f8a1ed8c7aaf8bbddfce2d3a22ee4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8dcd0fdc032715ea0a6f41d9a9429ffd65ed2be6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"b470561a8529c954248c451f23e45237cbcdc9e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"48b7c47e30c275673d5d2835de03be89e25a5c03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"aceadd60a38a722d008c8eab21ff591fdcb34f62","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"3f090a986ae89fabe3e9d881e073dc1014b29141","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"6fd44dc4ec13e06f1658bd6585b7bc0205f8446a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"2760aa799ac27e02a29b73f668ae0100bfbf6c6a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"877970ab89e79656b9258db498e3e78fcc7940bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"85062695b9ec35400ac3c9789e16784977e53a4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e5753f34dd23ad8b4baf445656435352a1885fd4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"a66a008880b8535ee3fcf4af83b45d0b95cc796d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d061a5a1c3bc1e99ac8f9934ea2dd16d9ddb9ddf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ae856ab26fcc94b4b46941041b6d3f6f3971943c","trusted":true},"execution_count":null,"outputs":[]}]}